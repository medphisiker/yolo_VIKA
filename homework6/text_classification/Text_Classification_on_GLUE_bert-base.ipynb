{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cRE8IbIrIV"
      },
      "source": [
        "ÐžÑÐ½Ð¾Ð²Ð°Ð½Ð¾ Ð½Ð° Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¼ Ð½Ð¾ÑƒÑ‚Ð±ÑƒÐºÐµ Ð¾Ñ‚ Optinum ([ÑÑÑ‹Ð»ÐºÐ°](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_quantization_ort.ipynb)).\n",
        "\n",
        "ÐŸÐµÑ€ÐµÐ´ÐµÐ»Ð°Ð½Ð¾ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ `Quick start`([ÑÑÑ‹Ð»ÐºÐ°](https://huggingface.co/docs/optimum/quicktour#onnx-runtime))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantizing a model with ONNX Runtime for text classification tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amVwvenQyAX2"
      },
      "source": [
        "This notebook shows how to apply different post-training quantization approaches such as static and dynamic quantization using [ONNX Runtime](https://onnxruntime.ai), for any tasks of the GLUE benchmark. This is made possible thanks to ðŸ¤— [Optimum](https://github.com/huggingface/optimum), an extension of ðŸ¤— [Transformers](https://github.com/huggingface/transformers), providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardwares.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYAw66NVyAX2"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers, ðŸ¤— Datasets and ðŸ¤— Optimum. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W78FNBvmyHmb",
        "outputId": "2c301293-326c-44a1-b28f-c5d9e6e56b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optimum[onnxruntime-gpu] in /usr/local/lib/python3.10/dist-packages (1.13.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (1.12)\n",
            "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (4.34.1)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (2.1.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (1.23.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (0.17.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (2.14.5)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (1.14.1)\n",
            "Requirement already satisfied: onnxruntime-gpu>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (1.16.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (3.20.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from optimum[onnxruntime-gpu]) (0.23.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum[onnxruntime-gpu]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime-gpu]) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime-gpu]) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu>=1.11.0->optimum[onnxruntime-gpu]) (23.5.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime-gpu]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime-gpu]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->optimum[onnxruntime-gpu]) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime-gpu]) (0.1.99)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->optimum[onnxruntime-gpu]) (5.9.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum[onnxruntime-gpu]) (10.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate->optimum[onnxruntime-gpu]) (0.18.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum[onnxruntime-gpu]) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum[onnxruntime-gpu]) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->optimum[onnxruntime-gpu]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->optimum[onnxruntime-gpu]) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[onnxruntime-gpu]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum[onnxruntime-gpu]) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[onnxruntime-gpu]) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install optimum[onnxruntime-gpu]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JjWDQacyAX3"
      },
      "source": [
        "Make sure your version of ðŸ¤— Optimum is at least 1.1.0 since the functionality was introduced in that version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq2-aUw0yAX3",
        "outputId": "4b2e315f-b830-480f-f893-ebfdd4287fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.2\n"
          ]
        }
      ],
      "source": [
        "from optimum.version import __version__\n",
        "\n",
        "print(__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "task = \"sst2\"\n",
        "model_checkpoint = \"textattack/bert-base-uncased-SST-2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the dataset and get the metric we need to use for evaluation. This can be easily done with the functions `load_dataset` and `load_metric`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKx2zKs5IrIq"
      },
      "source": [
        "`load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "5d31e13d-f370-48db-a550-baccbb6e7e19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-7ea79d66af03>:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(\"glue\", actual_task)\n"
          ]
        }
      ],
      "source": [
        "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
        "validation_split = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
        "eval_dataset = load_dataset(\"glue\", actual_task, split=validation_split)\n",
        "metric = load_metric(\"glue\", actual_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOCrQwPoIrJG"
      },
      "source": [
        "Note that `load_metric` has loaded the proper metric associated to your task, which is:\n",
        "\n",
        "- for CoLA: [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)\n",
        "- for MNLI (matched or mismatched): Accuracy\n",
        "- for MRPC: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for QNLI: Accuracy\n",
        "- for QQP: Accuracy and [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
        "- for RTE: Accuracy\n",
        "- for SST-2: Accuracy\n",
        "- for STS-B: [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) and [Spearman's_Rank_Correlation_Coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)\n",
        "- for WNLI: Accuracy\n",
        "\n",
        "so the metric object only computes the one(s) needed for your task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "To preprocess our dataset, we will need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fyGdtK9oIrJM"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"cola\": (\"sentence\", None),\n",
        "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "    \"qnli\": (\"question\", \"sentence\"),\n",
        "    \"qqp\": (\"question1\", \"question2\"),\n",
        "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "    \"sst2\": (\"sentence\", None),\n",
        "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer than what the model selected can handle will be truncated to the maximum length accepted by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VToHOmcXyAX6"
      },
      "outputs": [],
      "source": [
        "sentence1_key, sentence2_key = task_to_keys[task]\n",
        "\n",
        "def preprocess_function(examples, tokenizer):\n",
        "    args = (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "    return tokenizer(*args, padding=\"max_length\", max_length=128, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFecsHcjyAX7"
      },
      "source": [
        "## Applying quantization on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5EcaqqXyAX7"
      },
      "source": [
        "We can set our `quantization_approach` to either `dynamic` or `static` in order to apply respectively dynamic and static quantization.\n",
        "- Post-training static quantization : introduces an additional calibration step where data is fed through the network in order to compute the activations quantization parameters.\n",
        "- Post-training dynamic quantization : dynamically computes activations quantization parameters based on the data observed at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rYXvX7sqyAX7"
      },
      "outputs": [],
      "source": [
        "QUANTIZATION_APPROACH = [\"dynamic\", \"static\"]\n",
        "\n",
        "quantization_approach = \"static\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lLyewJ7yAX7"
      },
      "source": [
        "First, let's create the output directory where the resulting quantized model will be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vWX7gYBAyAX8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "output_dir = f\"{model_name}-{quantization_approach}-quantization\"\n",
        "save_dir = f\"{model_name}\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2E-xkrryAX8"
      },
      "source": [
        "We will use the [ðŸ¤— Optimum](https://github.com/huggingface/optimum) library to instantiate an `ORTQuantizer`, which will take care of the quantization process. To instantiate an `ORTQuantizer`, we need to provide a path to a converted ONNX checkpoint or instance of a `ORTModelForXXX`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If3IBaO6yAX8",
        "outputId": "8860a8d1-4cad-4ac3-d8bc-6bcda43debb5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Framework not specified. Using pt to export to ONNX.\n",
            "Using the export variant default. Available variants are:\n",
            "\t- default: The default ONNX variant.\n",
            "Using framework PyTorch: 2.1.0+cu118\n",
            "Overriding 1 configuration item(s)\n",
            "\t- use_cache -> False\n"
          ]
        }
      ],
      "source": [
        "# load model and save with tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n",
        "\n",
        "# Loading Model from the Hub and convert to ONNX\n",
        "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Save the ONNX model and tokenizer\n",
        "ort_model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Create a quantizer from a ORTModelForXXX\n",
        "quantizer = ORTQuantizer.from_pretrained(ort_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qtCbjP0yAX8"
      },
      "source": [
        "We also need to create an `QuantizationConfig` instance, which is the configuration handling the ONNX Runtime quantization related parameters.\n",
        "\n",
        "* We set `per_channel` to `False` in order to apply per-tensor quantization on the weights. As opposed to per-channel quantization, which introduces one set of quantization parameters per channel, per-tensor quantization means that there will be one set of quantization parameters per tensor.\n",
        "* We set the number of samples `num_calibration_samples` to use for the calibration step resulting from static quantization to `40`.\n",
        "* `operators_to_quantize` is used to specify the types of operators to quantize, here we want to quantize all the network's fully connected and embedding layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4M1e0RbcyAX8"
      },
      "outputs": [],
      "source": [
        "from optimum.onnxruntime.configuration import QuantizationConfig, AutoCalibrationConfig\n",
        "from optimum.onnxruntime.quantization import QuantFormat, QuantizationMode, QuantType\n",
        "\n",
        "per_channel = False\n",
        "num_calibration_samples = 40\n",
        "operators_to_quantize = [\"MatMul\", \"Add\", \"Gather\"]\n",
        "apply_static_quantization = quantization_approach == \"static\"\n",
        "\n",
        "qconfig = QuantizationConfig(\n",
        "    is_static=apply_static_quantization,\n",
        "    format=QuantFormat.QDQ if apply_static_quantization else QuantFormat.QOperator,\n",
        "    mode=QuantizationMode.QLinearOps if apply_static_quantization else QuantizationMode.IntegerOps,\n",
        "    activations_dtype=QuantType.QInt8 if apply_static_quantization else QuantType.QUInt8,\n",
        "    weights_dtype=QuantType.QInt8,\n",
        "    per_channel=per_channel,\n",
        "    operators_to_quantize=operators_to_quantize,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uako52RmyAX8"
      },
      "source": [
        "When applying static quantization, we need to perform a calibration step where the activations quantization ranges are computed. This additionnal step should only be performed in the case of static quantization and not for dynamic quantization.\n",
        "Because the quantization of certain nodes often results in degradation in accuracy, we create an instance of `QuantizationPreprocessor` to determine the nodes to exclude when applying static quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfjAEvOjyAX8",
        "outputId": "ed182c7a-a3f2-4d22-9a07-c557cd9f8816"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=False' instead.\n",
            "  warnings.warn(\n",
            "Using static quantization schema (dataset: glue, method: CalibrationMethod.MinMax)\n",
            "Creating calibrator: CalibrationMethod.MinMax(CalibrationConfig(dataset_name='glue', dataset_config_name='sst2', dataset_split='train', dataset_num_samples=40, method=<CalibrationMethod.MinMax: 0>, num_bins=None, num_quantized_bins=None, percentile=None, moving_average=False, averaging_constant=0.01))\n",
            "Collecting tensors statistics...\n",
            "Computing calibration ranges\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "from transformers import AutoTokenizer\n",
        "from optimum.onnxruntime.preprocessors import QuantizationPreprocessor\n",
        "from optimum.onnxruntime.preprocessors.passes import (\n",
        "    ExcludeGeLUNodes,\n",
        "    ExcludeLayerNormNodes,\n",
        "    ExcludeNodeAfter,\n",
        "    ExcludeNodeFollowedBy,\n",
        ")\n",
        "\n",
        "# Load tokenizer for preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "ranges = None\n",
        "quantization_preprocessor = None\n",
        "if apply_static_quantization:\n",
        "    # Create the calibration dataset used for the calibration step\n",
        "    calibration_dataset = quantizer.get_calibration_dataset(\n",
        "        \"glue\",\n",
        "        dataset_config_name=actual_task,\n",
        "        preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
        "        num_samples=num_calibration_samples,\n",
        "        dataset_split=\"train\",\n",
        "    )\n",
        "    calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)\n",
        "    # Perform the calibration step: computes the activations quantization ranges\n",
        "    ranges = quantizer.fit(\n",
        "        dataset=calibration_dataset,\n",
        "        calibration_config=calibration_config,\n",
        "        operators_to_quantize=qconfig.operators_to_quantize,\n",
        "    )\n",
        "    quantization_preprocessor = QuantizationPreprocessor()\n",
        "    # Exclude the nodes constituting LayerNorm\n",
        "    quantization_preprocessor.register_pass(ExcludeLayerNormNodes())\n",
        "    # Exclude the nodes constituting GELU\n",
        "    quantization_preprocessor.register_pass(ExcludeGeLUNodes())\n",
        "    # Exclude the residual connection Add nodes\n",
        "    quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Add\", \"Add\"))\n",
        "    # Exclude the Add nodes following the Gather operator\n",
        "    quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Gather\", \"Add\"))\n",
        "    # Exclude the Add nodes followed by the Softmax operator\n",
        "    quantization_preprocessor.register_pass(ExcludeNodeFollowedBy(\"Add\", \"Softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxVdCVMsyAX8"
      },
      "source": [
        "Finally, we export the quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGhSOpIpyAX8",
        "outputId": "5d49cc2f-1b04-4c33-9642-a95b2e70df3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating static quantizer: QDQ (mode: QLinearOps, schema: s8/s8, channel-wise: False)\n",
            "Preprocessor detected, collecting nodes to include/exclude\n",
            "Quantizing model...\n",
            "Saving quantized model at: bert-base-uncased-SST-2-static-quantization (external data format: False)\n",
            "Configuration saved in bert-base-uncased-SST-2-static-quantization/ort_config.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('bert-base-uncased-SST-2-static-quantization')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "quantizer.quantize(\n",
        "    save_dir=output_dir,\n",
        "    calibration_tensors_range=ranges,\n",
        "    quantization_config=qconfig,\n",
        "    preprocessor=quantization_preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUKBvMx-yAX8"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVBO-uFtyAX9"
      },
      "source": [
        "To evaluate our resulting quantized model we need to define how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits (our just squeeze the last axis in the case of STS-B).\n",
        "\n",
        "The metric chosen to evaluate the quantized model's performance will be Matthews correlation coefficient (MCC) for CoLA, Pearson correlation coefficient (PCC) for STS-B and accuracy for any other tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EVmRYT0ryAX9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    if task != \"stsb\":\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "    else:\n",
        "        predictions = predictions[:, 0]\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNEJNsIXyAX9"
      },
      "source": [
        "Then to apply the preprocessing on all the sentences (or pairs of sentences) of our validation dataset, we just use the `map` method of our `dataset` object that was earlier created. This will apply the `preprocess_function` function on all the elements of our validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "53925e54457e4acc90e9a492187251a8",
            "c2414c7389b74aebb06147081045233c",
            "e15dde2bd06043bbaf3f8dad4d6d6fd7",
            "bd6ef2b48fb74b418a53246513091185",
            "e34ebd8dfc584043b470db2c298fe436",
            "f59dcdea97e54ec0bf1554671a6345be",
            "07437d5747864dc2b0f6cbf70a24c171",
            "c9c52c8ede0e4f64849bf262254eda53",
            "223c2a1bac1e4648854e27270abe6be8",
            "276a819d86bf49b1a49ff540cfa63ca6",
            "3e423da2341245219f92ac9849bfad3f"
          ]
        },
        "id": "45E2GYoByAX9",
        "outputId": "baa1eccf-3658-428e-b458-65e5880d6c32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53925e54457e4acc90e9a492187251a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_dataset = eval_dataset.map(partial(preprocess_function, tokenizer=tokenizer), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymsR-2quyAYA"
      },
      "source": [
        "Finally, to estimate the drop in performance resulting from quantization, we are going to perform an evaluation step for both models (before and after quantization). In order to perform the latter, we will need to instantiate an `ORTModel` and thus need:\n",
        "\n",
        "* The path of the model to evaluate.\n",
        "* The dataset to use for the evaluation step.\n",
        "* The model's ONNX configuration `onnx_config` associated to the model. This instance of  `OnnxConfig` describes how to export the model through the ONNX format.\n",
        "* The function that will be used to compute the evaluation metrics `compute_metrics` that was defined previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sfDyGY7WMBRo"
      },
      "outputs": [],
      "source": [
        "# Ð¾ÑÑ‚Ð°Ð²Ð¸Ð¼ Ð¿Ð¾Ð´Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ð° Ð¸Ð· 200 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð²\n",
        "eval_dataset = eval_dataset.select(range(200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bai4LPDhVyi8"
      },
      "source": [
        "# Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JqVS4ySBZvQu"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kVviRfWUMdNF"
      },
      "outputs": [],
      "source": [
        "# Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð¸ÑÑ…Ð¾Ð´Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· ONNX-Ñ„Ð°Ð¹Ð»Ð°\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "save_directory='bert-base-uncased-SST-2'\n",
        "file_name='model.onnx'\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=file_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RJ4YtKd5NMgi"
      },
      "outputs": [],
      "source": [
        "# Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "map_classes = {'LABEL_0': 0,\n",
        "               'LABEL_1': 1}\n",
        "\n",
        "def make_prediction(classifier, eval_dataset, map_classes):\n",
        "    results = classifier(eval_dataset['sentence'])\n",
        "    pred_labels = [map_classes[result['label']] for result in results]\n",
        "    return pred_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw4eMclRNN-q",
        "outputId": "fcdf5ba2-14a2-4b81-94eb-a9792bb3e856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ð’Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° 26.16 ÑÐµÐºÑƒÐ½Ð´\n"
          ]
        }
      ],
      "source": [
        "# Ð·Ð°ÑÐµÐºÐ°ÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°\n",
        "start_time = perf_counter()\n",
        "\n",
        "# Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ\n",
        "pred_labels = make_prediction(classifier, eval_dataset, map_classes)\n",
        "\n",
        "end_time = perf_counter()\n",
        "print(f\"Ð’Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° {(end_time - start_time):.2f} ÑÐµÐºÑƒÐ½Ð´\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOArXZ0pPF2w",
        "outputId": "af3c3248-ea9b-4027-b985-3989c257d3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracy Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 0.915\n"
          ]
        }
      ],
      "source": [
        "# Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "pure_model_acc = accuracy_score(eval_dataset['label'], pred_labels)\n",
        "print(f'Acuracy Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {pure_model_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWI5g55BXjaf"
      },
      "source": [
        "# Ð’Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð¼ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Ð¿Ð¾ÑÐ»Ðµ ÑÑ‚Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "c-G-ozDUXrMy"
      },
      "outputs": [],
      "source": [
        "# Ð¡Ð¾Ð·Ð´Ð°Ð´Ð¸Ð¼ Ð¸ÑÑ…Ð¾Ð´Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· ONNX-Ñ„Ð°Ð¹Ð»Ð°\n",
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "save_directory='bert-base-uncased-SST-2-static-quantization'\n",
        "file_name='model_quantized.onnx'\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=file_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_zOXF6pXy0t",
        "outputId": "8c8cc9ea-4464-40c9-bcd7-8d59e8f73ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ð’Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° 26.78 ÑÐµÐºÑƒÐ½Ð´\n"
          ]
        }
      ],
      "source": [
        "# Ð·Ð°ÑÐµÐºÐ°ÐµÐ¼ Ð²Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°\n",
        "start_time = perf_counter()\n",
        "\n",
        "# Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ»Ð°ÑÑ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ\n",
        "pred_labels = make_prediction(classifier, eval_dataset, map_classes)\n",
        "\n",
        "end_time = perf_counter()\n",
        "print(f\"Ð’Ñ€ÐµÐ¼Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° {(end_time - start_time):.2f} ÑÐµÐºÑƒÐ½Ð´\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhl8VC_KX2X3",
        "outputId": "b4bb965f-e066-437a-f01e-32fb25e3a643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acuracy ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 0.89\n"
          ]
        }
      ],
      "source": [
        "# Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "pure_model_acc = accuracy_score(eval_dataset['label'], pred_labels)\n",
        "print(f'Acuracy ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {pure_model_acc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvq4SgADYF0N"
      },
      "source": [
        "# Ð¡Ñ€Ð°Ð½Ð¸Ð¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹ ONNX-Ñ„Ð°Ð¹Ð»Ð¾Ð²"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bjky1wjPS0G",
        "outputId": "5cfa250c-3476-4e9e-dc41-e87cb98764bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ð Ð°Ð·Ð¼ÐµÑ€ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 417.90 ÐœÐ±\n"
          ]
        }
      ],
      "source": [
        "model_onnx_path = '/content/bert-base-uncased-SST-2/model.onnx'\n",
        "model_size = os.path.getsize(model_onnx_path) / (1024*1024)\n",
        "\n",
        "print(f'Ð Ð°Ð·Ð¼ÐµÑ€ Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {model_size:.2f} ÐœÐ±')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8pwinwtYxxK",
        "outputId": "9d992291-3285-4c11-c35f-a45e88dc4b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ 106.85 ÐœÐ±\n"
          ]
        }
      ],
      "source": [
        "model_onnx_quant_path = '/content/bert-base-uncased-SST-2-static-quantization/model_quantized.onnx'\n",
        "model_quant_size = os.path.getsize(model_onnx_quant_path) / (1024*1024)\n",
        "\n",
        "print(f'Ð Ð°Ð·Ð¼ÐµÑ€ ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ {model_quant_size:.2f} ÐœÐ±')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-hnsXjnyAYB"
      },
      "source": [
        "ÐžÑ‚Ð½Ð¾ÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIYxNwfGyAYB",
        "outputId": "f68a11a7-1ed8-4404-d11d-9cd9bab0140a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.91"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ð²Ñ‹Ð¸Ð³Ñ€Ñ‹Ñˆ Ð¿Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñƒ Ñ„Ð°Ð¹Ð»Ð° Ð² 4 Ñ€Ð°Ð·Ð°!\n",
        "round(model_size / model_quant_size, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1AtbnvdZO6f"
      },
      "source": [
        "# Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑ€Ð¸Ð¿Ñ‚Ð° ÐºÐ²Ð°Ð½Ñ‚Ð¸Ð·Ð°Ñ†Ð¸Ð¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPo9fwytyAYC",
        "outputId": "c01ba7cd-9073-4b37-cf27-3f9b171304a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/run_glue.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python run_glue.py \\\n",
        "    --model_name_or_path distilbert-base-uncased-finetuned-sst-2-english \\\n",
        "    --task_name sst2 \\\n",
        "    --optimization_level 1 \\\n",
        "    --do_eval \\\n",
        "    --output_dir /content/optimized_distilbert_sst2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "bddb99ecda5b40a820d97bf37f3ff3a89fb9dbcf726ae84d28624ac628a665b4"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07437d5747864dc2b0f6cbf70a24c171": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "223c2a1bac1e4648854e27270abe6be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "276a819d86bf49b1a49ff540cfa63ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e423da2341245219f92ac9849bfad3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53925e54457e4acc90e9a492187251a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2414c7389b74aebb06147081045233c",
              "IPY_MODEL_e15dde2bd06043bbaf3f8dad4d6d6fd7",
              "IPY_MODEL_bd6ef2b48fb74b418a53246513091185"
            ],
            "layout": "IPY_MODEL_e34ebd8dfc584043b470db2c298fe436"
          }
        },
        "bd6ef2b48fb74b418a53246513091185": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_276a819d86bf49b1a49ff540cfa63ca6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3e423da2341245219f92ac9849bfad3f",
            "value": " 872/872 [00:00&lt;00:00, 3933.61 examples/s]"
          }
        },
        "c2414c7389b74aebb06147081045233c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59dcdea97e54ec0bf1554671a6345be",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_07437d5747864dc2b0f6cbf70a24c171",
            "value": "Map: 100%"
          }
        },
        "c9c52c8ede0e4f64849bf262254eda53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e15dde2bd06043bbaf3f8dad4d6d6fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c52c8ede0e4f64849bf262254eda53",
            "max": 872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_223c2a1bac1e4648854e27270abe6be8",
            "value": 872
          }
        },
        "e34ebd8dfc584043b470db2c298fe436": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f59dcdea97e54ec0bf1554671a6345be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
